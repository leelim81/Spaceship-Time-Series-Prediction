{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "#import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "We are using this data from Kaggle which is very similar to the merchants' data from www.spaceship.com.sg \n",
    "\n",
    "https://www.kaggle.com/jagangupta/time-series-basics-exploring-traditional-ts/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = 'data/'\n",
    "sales = pd.read_csv(data_path + 'sales_train.csv.gz')\n",
    "item_cats = pd.read_csv(data_path + 'item_categories.csv')\n",
    "items = pd.read_csv(data_path + 'items.csv')\n",
    "shops = pd.read_csv(data_path + 'shops.csv')\n",
    "\n",
    "#Use subset for simplicity\n",
    "sales = sales[sales['shop_id'].isin([26, 27, 28])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "We want to predict the target(items sold) in next month. \n",
    "The key idea is that we can use previous months items sales data to predict next month sales.\n",
    "We view the data as monthly blocks based on 'date_block_num'. \n",
    "Initutively, items sales can be attribute to the months(seasonality) or the ability of the individual shops.\n",
    "In each monthly block, we want to explore this relationship of the item-month and item-shop.\n",
    "\n",
    "- Split the data into their monthly block number \n",
    "- For each month, we want to aggregrate total items sold per shop_id-item_id pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   shop_id  item_id  date_block_num  target  target_shop  target_item\n",
      "0       28     7738               0     4.0       7057.0         11.0\n",
      "1       28     7737               0    10.0       7057.0         16.0\n",
      "2       28     7770               0     6.0       7057.0         10.0\n",
      "3       28     7664               0     1.0       7057.0          1.0\n",
      "4       28     7814               0     2.0       7057.0          6.0\n"
     ]
    }
   ],
   "source": [
    "# Create \"grid\" with columns\n",
    "index_cols = ['shop_id', 'item_id', 'date_block_num']\n",
    "\n",
    "# For every month we create a grid from all shops/items combinations from that month\n",
    "grid = [] \n",
    "for block_num in sales['date_block_num'].unique():\n",
    "    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n",
    "    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n",
    "    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n",
    "\n",
    "# Turn the grid into a dataframe\n",
    "grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n",
    "\n",
    "# Groupby data to get shop-item-month aggregates\n",
    "gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n",
    "# Fix column names\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] \n",
    "# Join it to the grid\n",
    "all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n",
    "\n",
    "# Same as above but with shop-month aggregates\n",
    "gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})\n",
    "gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "# Same as above but with item-month aggregates\n",
    "gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})\n",
    "gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\n",
    "all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n",
    "\n",
    "print all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Encoding\n",
    "Use target encoding of lagging sales data so as to encode seasonality for the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'shop_id', u'item_id', u'date_block_num', u'target', u'target_shop',\n",
      "       u'target_item', u'target_lag_1', u'target_item_lag_1',\n",
      "       u'target_shop_lag_1', u'target_lag_2', u'target_item_lag_2',\n",
      "       u'target_shop_lag_2', u'target_lag_3', u'target_item_lag_3',\n",
      "       u'target_shop_lag_3', u'target_lag_4', u'target_item_lag_4',\n",
      "       u'target_shop_lag_4', u'target_lag_5', u'target_item_lag_5',\n",
      "       u'target_shop_lag_5', u'target_lag_12', u'target_item_lag_12',\n",
      "       u'target_shop_lag_12'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# List of columns that we will use to create lags\n",
    "cols_to_rename = list(all_data.columns.difference(index_cols)) \n",
    "\n",
    "shift_range = [1, 2, 3, 4, 5, 12]\n",
    "\n",
    "for month_shift in shift_range:\n",
    "    train_shift = all_data[index_cols + cols_to_rename].copy()\n",
    "    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n",
    "    \n",
    "    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n",
    "    train_shift = train_shift.rename(columns=foo)\n",
    "\n",
    "    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n",
    "\n",
    "del train_shift\n",
    "# Remove the 1st 12 months because the are no previous 12 months lagging data\n",
    "all_data = all_data[all_data['date_block_num'] >= 12] \n",
    "\n",
    "# List of all lagged features\n",
    "fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n",
    "# We will drop these at fitting stage\n",
    "to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n",
    "\n",
    "# Category for each item\n",
    "item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n",
    "\n",
    "print all_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test split\n",
    "We will treat last month data as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test `date_block_num` is 33\n"
     ]
    }
   ],
   "source": [
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "last_block = dates.max()\n",
    "print('Test `date_block_num` is %d' % last_block)\n",
    "\n",
    "dates_train = dates[dates <  last_block]\n",
    "dates_test  = dates[dates == last_block]\n",
    "\n",
    "X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)\n",
    "X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)\n",
    "\n",
    "y_train = all_data.loc[dates <  last_block, 'target'].values\n",
    "y_test =  all_data.loc[dates == last_block, 'target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First level models \n",
    "We will implement a stacking scheme. We have a time component here, so we will use KFold scheme in time series. We always use first level models to build two datasets: test meta-features and 2-nd level train meta-features. \n",
    "We first get test meta-features here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R-squared for linreg is 0.743144\n",
      "Test R-squared for LightGBM is 0.707822\n",
      "Test R-squared for RandomForest is 0.667004\n",
      "(3354, 3)\n"
     ]
    }
   ],
   "source": [
    "#linear regression \n",
    "model_lr = LinearRegression()\n",
    "model_lr.fit(X_train.values, y_train)\n",
    "pred_lr = model_lr.predict(X_test.values)\n",
    "print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))\n",
    "\n",
    "#LightGBM\n",
    "lgb_params = {\n",
    "               'feature_fraction': 0.75,\n",
    "               'metric': 'rmse',\n",
    "               'nthread':1, \n",
    "               'min_data_in_leaf': 2**7, \n",
    "               'bagging_fraction': 0.75, \n",
    "               'learning_rate': 0.03, \n",
    "               'objective': 'mse', \n",
    "               'bagging_seed': 2**7, \n",
    "               'num_leaves': 2**7,\n",
    "               'bagging_freq':1,\n",
    "               'verbose':0 \n",
    "              }\n",
    "model_lgb = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)\n",
    "pred_lgb = model_lgb.predict(X_test)\n",
    "print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))\n",
    "\n",
    "#RandomForest\n",
    "model_forest = RandomForestRegressor(max_depth=3, random_state=42)\n",
    "model_forest.fit(X_train, y_train)\n",
    "pred_forest = model_forest.predict(X_test)\n",
    "print('Test R-squared for RandomForest is %f' % r2_score(y_test, pred_forest))\n",
    "\n",
    "#concatenate test predictions to get test meta-features\n",
    "X_test_level2 = np.c_[pred_lr, pred_lgb,pred_forest] \n",
    "print X_test_level2.shape\n",
    "#print np.corrcoef([pred_lr, pred_lgb,pred_forest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train meta-features\n",
    "#### KFold scheme in time series\n",
    "We cannot do normal KFold in time series because we need to be careful not to be using future data to validate past data.\n",
    "\n",
    "Split the train data into chunks of duration T. Select first M chunks.\n",
    "Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on. After that, use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
    "\n",
    "We have data for month 12 to 32(first 12 months was removed earlier). We use M=15. For the first fold, we will used month 12 to 26 in order to predict for month 27. The same logic apply for months 28, 29, 30, 31, 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34404,)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "# That is how we get target for the 2nd level dataset\n",
    "y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n",
    "\n",
    "pd_stack = pd.DataFrame(index=all_data.index)\n",
    "dates_train_level2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing block: 27  start_index: 0  last_index: 6438\n",
      "Processing block: 28  start_index: 6438  last_index: 13242\n",
      "Processing block: 29  start_index: 13242  last_index: 19935\n",
      "Processing block: 30  start_index: 19935  last_index: 26409\n",
      "Processing block: 31  start_index: 26409  last_index: 30027\n",
      "Processing block: 32  start_index: 30027  last_index: 34404\n",
      "[ 1.50291139  1.36876467  1.32128003]\n"
     ]
    }
   ],
   "source": [
    "# And here we create 2nd level feeature matrix, init it with zeros first\n",
    "X_train_level2 = np.zeros([y_train_level2.shape[0], 3])\n",
    "\n",
    "# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts \n",
    "dates = all_data['date_block_num']\n",
    "\n",
    "#Building metafeatures which are the predicted output from different models\n",
    "last_index = 0\n",
    "for cur_block_num in [27, 28, 29, 30, 31, 32]: \n",
    "        \n",
    "    #Split `X_train` into parts\n",
    "    X_train_level1 = all_data.loc[dates <  cur_block_num].drop(to_drop_cols, axis=1)\n",
    "    X_valid_level1 =  all_data.loc[dates == cur_block_num].drop(to_drop_cols, axis=1)\n",
    "\n",
    "    y_train_level1 = all_data.loc[dates <  cur_block_num, 'target'].values\n",
    "    y_valid_level1 =  all_data.loc[dates == cur_block_num, 'target'].values\n",
    "\n",
    "    start_index = last_index\n",
    "    last_index += y_valid_level1.shape[0]\n",
    "    print \"Processing block:\",cur_block_num, \" start_index:\", start_index, \" last_index:\", last_index\n",
    "\n",
    "    #Fit linear regression\n",
    "    model_lr.fit(X_train_level1.values, y_train_level1)\n",
    "    pred_lr = model_lr.predict(X_valid_level1.values)\n",
    "    X_train_level2[start_index:last_index,0] = pred_lr\n",
    " \n",
    "    #Fit lightGBM\n",
    "    model_lbg = lgb.train(lgb_params, lgb.Dataset(X_train_level1, label=y_train_level1), 100)\n",
    "    pred_lgb = model_lbg.predict(X_valid_level1)\n",
    "    X_train_level2[start_index:last_index,1] = pred_lgb\n",
    "    \n",
    "    #Fit Random Forest\n",
    "    model_forest.fit(X_train_level1.values, y_train_level1)\n",
    "    pred_forest = model_forest.predict(X_valid_level1.values)\n",
    "    X_train_level2[start_index:last_index,2] = pred_forest\n",
    "    \n",
    "print X_train_level2.mean(axis=0) # [1.50291139  1.36876467 1.32128003]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "Fit a linear regression model with the meta-features. Use this new level-2 meta model to predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R-squared for stacking is 0.641681\n",
      "Test R-squared for stacking is 0.759779\n"
     ]
    }
   ],
   "source": [
    "#Fit model\n",
    "model_level2 = LinearRegression()\n",
    "model_level2.fit(X_train_level2, y_train_level2)\n",
    "\n",
    "#Train result\n",
    "train_preds = model_level2.predict(X_train_level2) \n",
    "r2_train_stacking = r2_score(y_train_level2, train_preds)\n",
    "print('Train R-squared for stacking is %f' % r2_train_stacking)\n",
    "\n",
    "#test result\n",
    "test_preds = model_level2.predict(X_test_level2)\n",
    "r2_test_stacking = r2_score(y_test, test_preds)\n",
    "print('Test R-squared for stacking is %f' % r2_test_stacking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve a score of 0.75 for test data. This can be further improved by stacking different models and exploring more external data which affect time series. One good example is public holidays which can affect sales of certain items a lot. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
